# Knowledge Distillation Training Config for CIFAR10
# Same methodology as distill_cached_random, but training on CIFAR10
#
# DIAGNOSTIC STEPS:
# Step 1: Verify representation consistency - automatically checked at training start
# Step 2: CLS-only KD - set distill_loss_weights.patch: 0.0
# Step 3: Minimal augmentation - set use_minimal_aug: true
# Step 4: Supervised baseline - not implemented (requires separate training script)

# Training hyperparameters
batch_size: 64
num_epochs: 20
learning_rate: 0.0005  # 5e-4
weight_decay: 0.04
warmup_epochs: 10
min_lr: 1e-6

# Step-capped training (optional - limits batches per epoch)
# CIFAR10 has 50K training images, so ~781 batches per epoch with batch_size=64
# max_steps_per_epoch: null  # Use all batches (no cap)

# Distillation loss weights
# DIAGNOSTIC: For CLS-only KD experiment, set patch: 0.0
# This tests if patch loss is interfering with CLS learning
distill_loss_weights:
  cls: 1.0      # CLS token loss weight
  patch: 0.5    # Patch token loss weight (set to 0.0 for CLS-only experiment)

# Data augmentation (simplified for speed)
use_multi_crop: false  # Use simple single-crop transform
use_local_crops: false  # Disable local crops (major speedup)
# DIAGNOSTIC Step 3: Set use_minimal_aug: true for minimal augmentation (resize + flip only, no crop/blur)
use_minimal_aug: false  # Minimal augmentation for debugging (resize + flip only)

# Teacher feature caching (optional, major speedup)
cache_teacher_features: false  # Set to true to cache teacher features
teacher_feature_dir: "/scratch/$USER/ssl_project/cache/features"

# Optimization settings
compile_student: true   # Compile only student (never compile teacher)
use_fused_adamw: true   # Use fused AdamW optimizer

# DataLoader settings (optimized for Slurm)
num_workers: 2  # Reduced to prevent OOM (was 4)
persistent_workers: false  # Must be false to avoid KeyError
prefetch_factor: 1  # Reduced to prevent OOM (was 2)
pin_memory: true

# Checkpointing
checkpoint_dir: "/scratch/$USER/Nov_14_distill/checkpoints_cifar10"
save_every: 100  # Save checkpoint every N steps (0 = only at end of epoch)

# SSL (Self-Supervised Learning) configuration
# Adds Barlow Twins loss on top of KD to improve feature discriminativeness
ssl:
  enabled: false  # Set to true to enable SSL (Barlow Twins)
  type: "barlow"  # SSL loss type (currently only "barlow" supported)
  weight: 0.5     # Weight for SSL loss: L_total = L_kd + weight * L_ssl
  barlow_lambd: 0.005  # Lambda for Barlow Twins off-diagonal term (default: 5e-3)
  proj_hidden_dim: 1024  # Hidden dimension for projection head
  proj_out_dim: 256      # Output dimension for projection head
