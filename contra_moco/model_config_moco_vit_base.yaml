# MoCo-v3 Model Config for Vision Transformer Base (ViT-B/8)
# NOTE: ViT-Base/8 has ~86M trainable parameters (encoder_q + proj_q)
# This is under the 100M limit but close - monitor memory usage
# If you need more headroom, consider reducing proj_hidden_dim in train_config_moco.yaml
# Patch size 8 gives 144 tokens for 96x96 images: (96/8)^2 = 144

# Backbone configuration
backbone_name: "vit_base_patch8_224"  # ViT-B/8 from timm (~86M trainable params, under 100M limit)
backbone_type: "vit"  # Explicitly set to "vit" (or "auto" for auto-detection)
image_size: 96  # Image size for training (CIFAR-10 upscaled or custom dataset)

# Model architecture
use_cls_token: true  # Use CLS token for ViT (always true for ViT)

