# Data loading configuration (optimized for random subset sampling per epoch)

dataset_name: "tsbpp/fall2025_deeplearning"
image_size: 96  # Match original image size (96x96) - student operates at native resolution
num_workers: 2  # Reduced to prevent OOM
persistent_workers: false  # Must be false
prefetch_factor: 1  # Reduced to prevent OOM
pin_memory: true

# Cache configuration (2-stage pipeline)
# Auto-detection: If cache exists, it will be used automatically
# Set to false to explicitly disable cached mode even if cache exists
use_cached_tensors: null  # null = auto-detect, true = force enable, false = force disable
cache_root: "/scratch/jj4252/Nov_14_distill/cache_images"  # Root directory for cached tensors
cache_image_size: 96  # Match original image size (96x96) - no need to cache larger
cache_dtype: "float32"  # Data type for cached tensors (float32 or float16)
cache_shard_size: 2048  # Number of samples per shard file

# Random subset sampling per epoch
# This enables training on large cached datasets (e.g., 250K images) by only
# processing a random subset per epoch (e.g., 30K images). Over many epochs,
# the model will see most/all of the cached data through stochastic sampling.
samples_per_epoch: 30000  # Number of random samples to use per epoch (~30K)
random_subset_seed: 42  # Random seed for subset sampling (for reproducibility)

# Subset limits for testing (optional - usually not needed with random sampling)
# max_shards: null  # Limit to first N shard files (for testing)
# max_samples: null  # Limit by total samples (for testing)

# Legacy compatibility (will be removed)
use_cached: false  # Deprecated: use use_cached_tensors instead
cache_dir: "/scratch/jj4252/Nov_14_distill/cache_images"  # Deprecated: use cache_root instead
