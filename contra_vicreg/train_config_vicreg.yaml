# VICReg Training Config
# Self-supervised learning with ViT-S/16 backbone

# Training mode
training_mode: "vicreg"

# Training hyperparameters
batch_size: 128  # Keep at 128 - VICReg's memory issue is from large proj_dim (2048) and covariance matrices [2048x2048], not just batch size
num_epochs: 200
learning_rate: 0.0002  # Further reduced to allow variance/covariance terms to prevent collapse
weight_decay: 0.0001  # 1e-4
warmup_epochs: 10
min_lr: 1e-6

# VICReg specific settings
# Config B (Anti-collapse): Aggressive variance/covariance to prevent collapse
vicreg:
  proj_dim: 2048           # Projection head output dimension
  proj_hidden_dim: 2048    # Hidden dimension in projection head
  lambda_invariance: 0.5   # Weight for invariance term (further reduced - let variance/covariance dominate)
  mu_variance: 50.0        # Weight for variance term (doubled - force higher variance)
  nu_covariance: 50.0      # Weight for covariance term (doubled - stronger decorrelation)
  gamma: 2.0               # Target standard deviation (increased from 1.0 - force higher variance)

# Data augmentation (VICReg style)
# Two views per image: RandomResizedCrop, RandomHorizontalFlip, ColorJitter, RandomGrayscale, GaussianBlur
use_vicreg_aug: true  # Use VICReg style augmentations

# Optimization settings
compile_model: true   # Compile model with torch.compile
use_fused_adamw: true # Use fused AdamW optimizer
max_grad_norm: 0.5    # Gradient clipping (keep at 0.5 for stability)

# DataLoader settings (reduced to prevent OOM)
num_workers: 2  # Reduced from 4 to match data_config and prevent OOM
persistent_workers: false
prefetch_factor: 1  # Reduced from 2 to prevent OOM
pin_memory: true

# Checkpointing
checkpoint_dir: "/scratch/$USER/Nov_14_distill/checkpoints_vicreg"
save_every: 0  # Save checkpoint only at end of epoch (0 = disable step-based saving)

