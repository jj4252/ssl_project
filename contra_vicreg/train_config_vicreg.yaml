# VICReg Training Config
# Self-supervised learning with ViT-S/16 backbone

# Training mode
training_mode: "vicreg"

# Training hyperparameters
batch_size: 128  # Keep at 128 - VICReg's memory issue is from large proj_dim (2048) and covariance matrices [2048x2048], not just batch size
num_epochs: 200
learning_rate: 0.0003  # Reverted to original (was 0.0002 in Config B)
weight_decay: 0.0001  # 1e-4
warmup_epochs: 10
min_lr: 1e-6

# VICReg specific settings
# Config A (Original - gave ~57% accuracy): Rebalanced loss to prevent feature collapse
vicreg:
  proj_dim: 2048           # Projection head output dimension
  proj_hidden_dim: 2048    # Hidden dimension in projection head
  lambda_invariance: 1.0  # Weight for invariance term (reverted from 0.5)
  mu_variance: 25.0        # Weight for variance term (reverted from 50.0)
  nu_covariance: 25.0      # Weight for covariance term (reverted from 50.0)
  gamma: 1.0               # Target standard deviation (reverted from 2.0)

# Data augmentation (VICReg style)
# Two views per image: RandomResizedCrop, RandomHorizontalFlip, ColorJitter, RandomGrayscale, GaussianBlur
use_vicreg_aug: true  # Use VICReg style augmentations

# Optimization settings
compile_model: true   # Compile model with torch.compile
use_fused_adamw: true # Use fused AdamW optimizer
max_grad_norm: 0.5    # Gradient clipping (keep at 0.5 for stability)

# DataLoader settings (reduced to prevent OOM)
num_workers: 2  # Reduced from 4 to match data_config and prevent OOM
persistent_workers: false
prefetch_factor: 1  # Reduced from 2 to prevent OOM
pin_memory: true

# Checkpointing
checkpoint_dir: "/scratch/$USER/Nov_14_distill/checkpoints_vicreg"
save_every: 0  # Save checkpoint only at end of epoch (0 = disable step-based saving)

